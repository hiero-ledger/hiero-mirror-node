# SPDX-License-Identifier: Apache-2.0

name: Acceptance

on:
  pull_request:
    branches: [main, release/**]
    paths:
      - "test/**"
  push:
    branches: [main, release/**]
    tags:
      - "v*"
  workflow_dispatch:
    inputs:
      branch:
        description: "Branch"
        required: true
        type: string

permissions:
  contents: read

defaults:
  run:
    shell: bash

jobs:
  solo:
    runs-on: hiero-mirror-node-linux-large
    timeout-minutes: 40
    env:
      SOLO_CLUSTER_NAME: solo-mirror-acceptance
      SOLO_NAMESPACE: solo
      SOLO_CLUSTER_SETUP_NAMESPACE: solo-mirror-acceptance-setup
      SOLO_DEPLOYMENT: solo-deployment
      OPERATOR_ID: 0.0.2
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@ec9f2d5744a09debf3a187a3f4f675c53b671911 # v2.13.0
        with:
          egress-policy: audit

      - name: Checkout
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Setup Node
        uses: actions/setup-node@49933ea5288caeca8642d1e84afbd3f7d6820020 # v4.4.0
        with:
          node-version: 21

      - name: Install Wget
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends wget

      - name: Install Helm
        uses: azure/setup-helm@b9e51907a09c216f16ebe8536097933489208112 # v4.3.0

      - name: Setup Kind
        uses: helm/kind-action@a1b0e391336a6ee6713a0583f8c6240d70863de3 # v1.12.0
        with:
          kubectl_version: v1.32.3

      - name: Install JDK
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00 # v4.7.1
        with:
          distribution: "temurin"
          java-version: 21

      - name: Install Solo CLI via npm
        run: |
          npm install -g @hashgraph/solo

      - name: Setup Solo Cluster
        run: |
          kind create cluster -n "${SOLO_CLUSTER_NAME}" || { echo "Failed to create Kind cluster"; exit 1; }
          solo init || { echo "Failed to initialize Solo"; exit 1; }
          solo cluster-ref connect --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --context kind-"${SOLO_CLUSTER_NAME}" || { echo "Failed to connect to cluster"; exit 1; }
          solo deployment create --deployment "${SOLO_DEPLOYMENT}" --namespace "${SOLO_NAMESPACE}" || { echo "Failed to create deployment"; exit 1; }
          solo deployment add-cluster --deployment "${SOLO_DEPLOYMENT}" --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --num-consensus-nodes 1 || { echo "Failed to add cluster to deployment"; exit 1; }
          solo cluster-ref setup --cluster-ref kind-"${SOLO_CLUSTER_NAME}" || { echo "Failed to setup cluster"; exit 1; }
          solo node keys --gossip-keys --tls-keys --deployment "${SOLO_DEPLOYMENT}" -i node1 || { echo "Failed to generate keys"; exit 1; }
          solo network deploy --deployment "${SOLO_DEPLOYMENT}" || { echo "Failed to deploy network"; exit 1; }
          solo node setup --deployment "${SOLO_DEPLOYMENT}" -i node1 || { echo "Failed to setup node"; exit 1; }
          solo node start --deployment "${SOLO_DEPLOYMENT}" -i node1 || { echo "Failed to start node"; exit 1; }

          solo account get --deployment "${SOLO_DEPLOYMENT}" --account-id "${OPERATOR_ID}" --private-key > account_get.log || { echo "Failed to get account"; exit 1; }
          OPERATOR_KEY=$(grep "privateKey" account_get.log | awk '{print $2}' | sed 's/"//g'| sed 's/,//g')  || { echo "Failed to get account"; exit 1; }
          rm account_get.log

          cat <<EOF > temp-values.yaml
          global:
            image:
              tag: "${{ github.base_ref }}"
              pullPolicy: IfNotPresent
          test:
            config:
              hiero:
                mirror:
                  test:
                    acceptance:
                      network: OTHER
                      operatorId: "${OPERATOR_ID}"
                      operatorKey: "${OPERATOR_KEY}"
          EOF

          solo mirror-node deploy --deployment "${SOLO_DEPLOYMENT}" -f=temp-values.yaml --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --pinger || { echo "Failed to deploy mirror node"; exit 1; }

          echo "Solo cluster setup completed successfully!"

      - name: Wait for services to be ready
        run: |
          echo "Waiting for services to be ready..."
          kubectl wait --for=condition=ready pod -l app=mirror -n "${SOLO_NAMESPACE}" --timeout=10m || echo "Some pods may not be ready, continuing with tests"

      - name: Get dynamic node information
        run: |
          echo "Getting dynamic node information from Solo network..."

          # Get the account ID from the network node pod
          SOLO_NODE_ACCOUNT_ID=$(kubectl get pod network-node1-0 -n "${SOLO_NAMESPACE}" -o jsonpath='{.metadata.labels.solo\.hedera\.com/account-id}' 2>/dev/null || echo "")

          if [ -z "$SOLO_NODE_ACCOUNT_ID" ]; then
            echo "Error: Could not retrieve account ID from network node"
            kubectl get pods -n "${SOLO_NAMESPACE}" | grep node
            exit 1
          fi

          echo "SOLO_NODE_ACCOUNT_ID=${SOLO_NODE_ACCOUNT_ID}" >> $GITHUB_ENV

      - name: Get service URLs
        run: |
          echo "Getting Kubernetes service URLs..."

          # Get service URLs using Kubernetes DNS names
          GRPC_URL="${SOLO_DEPLOYMENT}-grpc.${SOLO_NAMESPACE}.svc.cluster.local:5600"
          REST_URL="http://${SOLO_DEPLOYMENT}-rest.${SOLO_NAMESPACE}.svc.cluster.local"
          REST_JAVA_URL="http://${SOLO_DEPLOYMENT}-restjava.${SOLO_NAMESPACE}.svc.cluster.local"
          WEB3_URL="http://${SOLO_DEPLOYMENT}-web3.${SOLO_NAMESPACE}.svc.cluster.local"
          NETWORK_URL="network-node1-svc.${SOLO_NAMESPACE}.svc.cluster.local:50211"

          # Export for use in next steps
          echo "GRPC_URL=${GRPC_URL}" >> $GITHUB_ENV
          echo "REST_URL=${REST_URL}" >> $GITHUB_ENV
          echo "REST_JAVA_URL=${REST_JAVA_URL}" >> $GITHUB_ENV
          echo "WEB3_URL=${WEB3_URL}" >> $GITHUB_ENV
          echo "NETWORK_URL=${NETWORK_URL}" >> $GITHUB_ENV

      - name: Run acceptance tests
        run: |
          ./gradlew :test:acceptance --info \
            -Dcucumber.filter.tags=@acceptance \
            -Dhiero.mirror.test.acceptance.mirrorNodeAddress=${GRPC_URL} \
            -Dhiero.mirror.test.acceptance.network=OTHER \
            -Dhiero.mirror.test.acceptance.nodes[0].accountId=${SOLO_NODE_ACCOUNT_ID} \
            -Dhiero.mirror.test.acceptance.nodes[0].host=network-node1-svc.${SOLO_NAMESPACE}.svc.cluster.local \
            -Dhiero.mirror.test.acceptance.nodes[0].port=50211 \
            -Dhiero.mirror.test.acceptance.operatorId=${OPERATOR_ID} \
            -Dhiero.mirror.test.acceptance.operatorKey=${OPERATOR_KEY} \
            -Dhiero.mirror.test.acceptance.rest.baseUrl=${REST_URL} \
            -Dhiero.mirror.test.acceptance.rest-java.baseUrl=${REST_JAVA_URL} \
            -Dhiero.mirror.test.acceptance.web3.baseUrl=${WEB3_URL} \

      - name: Show Pod Logs on Failure
        if: ${{ failure() }}
        run: |
          echo "--------------------------------------------------"
          echo "Workflow failed. Collecting logs for debugging..."
          echo "--------------------------------------------------"

          echo "--- Describing all pods in namespace ${SOLO_NAMESPACE} ---"
          kubectl describe pods -n "${SOLO_NAMESPACE}" || echo "Could not describe pods"

          echo "--- Fetching logs for all pods in namespace ${SOLO_NAMESPACE} ---"
          for pod in $(kubectl get pods -n "${SOLO_NAMESPACE}" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo ""); do
            if [ -n "$pod" ]; then
              echo "--- Logs for pod: $pod ---"
              kubectl logs "$pod" -n "${SOLO_NAMESPACE}" --all-containers=true --tail=1000 || echo "Could not get logs for pod $pod"
            fi
          done

      - name: Cleanup
        if: always()
        run: |
          echo "Cleaning up resources..."
          kind delete cluster -n "${SOLO_CLUSTER_NAME}" || echo "Failed to delete cluster"
          rm -rf ~/.solo || echo "Failed to remove solo config"
          echo "Cleanup completed"
