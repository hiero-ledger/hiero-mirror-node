# SPDX-License-Identifier: Apache-2.0

name: End-to-end Tests

on:
  pull_request:
    branches:
      - "main"
      - "release/**"
    paths:
      - ".github/workflows/acceptance.yaml"
      - "test/**"
  push:
    branches:
      - "main"
      - "release/**"
    tags:
      - "v*"
  workflow_dispatch:
    inputs:
      branch:
        description: "Branch"
        required: true
        type: string

permissions:
  contents: read

defaults:
  run:
    shell: bash

jobs:
  acceptance:
    runs-on: hiero-mirror-node-linux-large
    strategy:
      fail-fast: false
      matrix:
        stream-type:
          - RECORD
          - BLOCK
    timeout-minutes: 40
    env:
      BLOCK_NODE_CHART_VERSION: v0.20.1
      CONSENSUS_VERSION: v0.67.0
      HELM_RELEASE_NAME: mirror-1
      MIRROR_NODE_VERSION: v0.141.0-rc1
      SOLO_CLUSTER_NAME: test
      SOLO_NAMESPACE: mirror
      SOLO_CLUSTER_SETUP_NAMESPACE: solo
      SOLO_DEPLOYMENT: solo-deployment
      SOLO_VERSION: v0.46.1
      TAG: pr-changes
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@f4a75cfd619ee5ce8d5b864b0d183aff3c69b55a # v2.13.1
        with:
          egress-policy: audit

      - name: Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          ref: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.branch || github.ref }}

      - name: Setup Node
        uses: actions/setup-node@2028fbc5c25fe9cf00d9f06a71cc4710d4507903 # v6.0.0
        with:
          node-version: 22

      - name: Install Helm
        uses: azure/setup-helm@1a275c3b69536ee54be43f2070a358922e12c8d4 # v4.3.1

      - name: Setup Kind
        uses: helm/kind-action@a1b0e391336a6ee6713a0583f8c6240d70863de3 # v1.12.0
        with:
          kubectl_version: v1.32.3

      - name: Install JDK
        uses: actions/setup-java@dded0888837ed1f317902acf8a20df0ad188d165 # v5.0.0
        with:
          distribution: "temurin"
          java-version: 21

      - name: Install Solo CLI via npm
        run: npm install -g @hashgraph/solo@${SOLO_VERSION}

      - name: Build Mirror Node Test JAR
        run: ./gradlew test:build -x test

      - name: Build Mirror Node Test Docker image
        run: docker build -t gcr.io/mirrornode/hedera-mirror-test:${TAG} ./test

      - name: Setup Solo Cluster
        run: |
          set -ex

          kind create cluster -n "${SOLO_CLUSTER_NAME}"
          kind load docker-image gcr.io/mirrornode/hedera-mirror-test:${TAG} -n "${SOLO_CLUSTER_NAME}"

          solo init
          solo cluster-ref config connect --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --context kind-"${SOLO_CLUSTER_NAME}"
          solo deployment config create --deployment "${SOLO_DEPLOYMENT}" --namespace "${SOLO_NAMESPACE}"
          solo deployment cluster attach --deployment "${SOLO_DEPLOYMENT}" --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --num-consensus-nodes 1
          solo cluster-ref config setup --cluster-ref kind-"${SOLO_CLUSTER_NAME}"

          if [ "${{ matrix.stream-type }}" = "BLOCK" ]; then
            solo block node add --deployment "${SOLO_DEPLOYMENT}" --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --chart-version "${BLOCK_NODE_CHART_VERSION}" --release-tag "${CONSENSUS_VERSION}"
          fi
          solo keys consensus generate --gossip-keys --tls-keys --deployment "${SOLO_DEPLOYMENT}" -i node1
          solo consensus network deploy --deployment "${SOLO_DEPLOYMENT}" --release-tag "${CONSENSUS_VERSION}"
          solo consensus node setup --deployment "${SOLO_DEPLOYMENT}" -i node1 --release-tag "${CONSENSUS_VERSION}"
          solo consensus node start --deployment "${SOLO_DEPLOYMENT}" -i node1

          if [ "${{ matrix.stream-type }}" = "BLOCK" ]; then
            cat <<EOF > mirror.yaml
          importer:
            env:
              HIERO_MIRROR_IMPORTER_BLOCK_NODES_0_HOST: 'block-node-1.${SOLO_NAMESPACE}.svc.cluster.local'
              SPRING_PROFILES_ACTIVE: 'blocknode'
          test:
            cucumberTags: "@acceptance and not @crud and not @equivalence and not @estimateprecompile and not @ethereum and not @historical and not @precompile and not @erc and not @ethcall"
            image:
              tag: ${TAG}
            annotations:
              helm.sh/hook-delete-policy: before-hook-creation
            env:
              HIERO_MIRROR_TEST_ACCEPTANCE_FEATURE_CONTRACTCALLLOCALESTIMATE: "true"
              HIERO_MIRROR_TEST_ACCEPTANCE_NETWORK: OTHER
              HIERO_MIRROR_TEST_ACCEPTANCE_WEB3_OPCODETRACER_ENABLED: "true"
            enabled: true
          web3:
            env:
              HIERO_MIRROR_WEB3_OPCODE_TRACER_ENABLED: "true"
          EOF

          else
            cat <<EOF > mirror.yaml
          test:
            image:
              tag: ${TAG}
            annotations:
              helm.sh/hook-delete-policy: before-hook-creation
            env:
              HIERO_MIRROR_TEST_ACCEPTANCE_FEATURE_CONTRACTCALLLOCALESTIMATE: "true"
              HIERO_MIRROR_TEST_ACCEPTANCE_NETWORK: OTHER
              HIERO_MIRROR_TEST_ACCEPTANCE_WEB3_OPCODETRACER_ENABLED: "true"
              HIERO_MIRROR_TEST_ACCEPTANCE_SKIPENTITIESCLEANUP: "true"
            enabled: true
          web3:
            env:
              HIERO_MIRROR_WEB3_OPCODE_TRACER_ENABLED: "true"
          EOF
          fi

          solo mirror node add --deployment "${SOLO_DEPLOYMENT}" -f mirror.yaml --cluster-ref kind-"${SOLO_CLUSTER_NAME}" --mirror-node-version "${MIRROR_NODE_VERSION}" --pinger

          echo "Solo cluster setup completed successfully!"

      - name: Run acceptance tests
        run: helm test "${HELM_RELEASE_NAME}" -n "${SOLO_NAMESPACE}" --logs --filter name="${HELM_RELEASE_NAME}-mirror-acceptance" --timeout 20m | tee output_log.txt

      - name: Parse acceptance tests logs and set k6 environment variables
        if: ${{ matrix.stream-type != 'BLOCK' }}
        run: |
          set -ex
          LOG_FILE="./output_log.txt"

          declare -A k6Map
          k6Map["PARENT"]="DEFAULT_CONTRACT_ADDRESS"
          k6Map["ERC"]="ERC_CONTRACT_ADDRESS"
          k6Map["ESTIMATE_PRECOMPILE"]="ESTIMATE_PRECOMPILE_CONTRACT"
          k6Map["PRECOMPILE"]="PRECOMPILE_CONTRACT"
          k6Map["PRECOMPILE"]="HTS_CONTRACT_ADDRESS"
          k6Map["BOB"]="PAYER_ACCOUNT"
          k6Map["ALICE"]="ACCOUNT_ADDRESS"
          k6Map["fungible"]="TOKEN_ADDRESS"
          k6Map["non_fungible"]="NON_FUNGIBLE_TOKEN_ADDRESS"

          for key in "${!k6Map[@]}"; do
            k6="${k6Map[$key]}"
            value=$(grep -oP "^${key}=(\K.+)$" "$LOG_FILE" | tail -n 1  || true)
            echo "${k6}=${value}" >> $GITHUB_ENV
          done

      - name: Install k6
        if: ${{ matrix.stream-type != 'BLOCK' }}
        uses: grafana/setup-k6-action@ffe7d7290dfa715e48c2ccc924d068444c94bde2 # v1

      - name: Port forward web3
        if: ${{ matrix.stream-type != 'BLOCK' }}
        run: |
          echo "Port forwarding 8080:80"
          kubectl port-forward "service/${HELM_RELEASE_NAME}-web3" -n "${SOLO_NAMESPACE}" 8080:80 &

      - name: Run k6 web3 tests
        if: ${{ matrix.stream-type != 'BLOCK' }}
        uses: grafana/run-k6-action@a15e2072ede004e8d46141e33d7f7dad8ad08d9d # v1
        env:
          AMOUNT: "0000000000000000000000000000000000000000000000000000000000000001"
          BASE_URL: "http://127.0.0.1:8080"
          DEFAULT_DURATION: "3s"
          DEFAULT_GRACEFUL_STOP: "1s"
          DEFAULT_VUS: 2
          KEY_TYPE: "0000000000000000000000000000000000000000000000000000000000000001"
          RUN_COMPLEX_TESTS: "false"
          RUN_ESTIMATE_TESTS: "false"
          RUN_MODIFICATION_TESTS: "false"
          RUN_WITH_VARIABLES: "true"
          SERIAL_NUMBER: "0000000000000000000000000000000000000000000000000000000000000001"
          WEB3_TEST_EXCLUDE: "^contractCall(Allowance|ApprovedForAll|BalanceOf|IsFrozen|IsKyc|Receive).*$"
        with:
          path: |
            ./tools/k6/src/web3/apis.js
          debug: false
          flags: --quiet

      - name: Show Pod Logs on Failure
        continue-on-error: true
        if: ${{ failure() }}
        run: |
          echo "--------------------------------------------------"
          echo "Workflow failed. Collecting logs for debugging..."
          echo "--------------------------------------------------"

          echo "--- Describing all pods in namespace ${SOLO_NAMESPACE} ---"
          kubectl describe pods -n "${SOLO_NAMESPACE}" || echo "Could not describe pods"

          echo "--- Fetching logs for all pods in namespace ${SOLO_NAMESPACE} ---"
          for pod in $(kubectl get pods -n "${SOLO_NAMESPACE}" -o name); do
            if [ -n "$pod" ]; then
              echo "--- Logs for pod: $pod ---"
              kubectl logs "$pod" -n "${SOLO_NAMESPACE}" --all-containers=true --tail=10000 || echo "Could not get logs for pod $pod"
            fi
          done

      - name: Cleanup
        continue-on-error: true
        if: always()
        run: kind delete cluster -n "${SOLO_CLUSTER_NAME}"
